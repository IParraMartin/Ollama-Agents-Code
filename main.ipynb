{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = Ollama(model='mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = Agent(\n",
    "    role=\"Researcher\",\n",
    "    goal=\"Using all your power and knowledge, find the best way to create a dataset to improve low-resource NLP models. This may entail crafting a dataset from scratch, or finding a way to augment an existing dataset.\",\n",
    "    backstory=\"You are a powerful researcher, with a lot of knowledge about NLP and machine learning. You are specially interested in how to improve the state of NLP for low-resource (scarce data) scenarios. You have a lot of resources at your disposal, and you are able to delegate tasks to others.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=ollama_model\n",
    ")\n",
    "\n",
    "supervisor = Agent(\n",
    "    role=\"Supervisor\",\n",
    "    goal=\"Using all your power and knowledge, supervise the researcher and make sure they are doing their job.\",\n",
    "    backstory=\"You are a supervisor. You are in charge of the researcher, and you have to make sure they are doing their job coherently.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=ollama_model\n",
    ")\n",
    "\n",
    "writer = Agent(\n",
    "    role=\"Writer\",\n",
    "    goal=\"Using all your power and knowledge, write a dataset with the information provided by the Researcher.\",\n",
    "    backstory=\"You are a writer. You are in charge of writing the dataset.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=ollama_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_1 = Task(description='Figure out the best way to create a dataset to improve low-resource NLP models.', agent=researcher)\n",
    "task_2 = Task(description='Supervise the researcher and make sure they are doing their job correctly.', agent=supervisor)\n",
    "task_3 = Task(description='Write a detailed methodology to improve the performance of LMs in data scarce scenarios.', agent=writer)\n",
    "\n",
    "crew = Crew(\n",
    "    agents=[researcher, supervisor, writer],\n",
    "    tasks=[task_1, task_2, task_3],\n",
    "    verbose=True,\n",
    "    allow_delegation=True,\n",
    "    process=Process.sequential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Agent: Researcher\n",
      "Starting Task: Figure out the best way to create a dataset to improve low-resource NLP models. ...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: Do I need to use a tool? No\n",
      "\n",
      "Final Answer: Creating a high-quality dataset for low-resource NLP models can be challenging, but there are several strategies that have been shown to be effective. Here are some ideas:\n",
      "\n",
      "1. Transfer Learning: Use pre-trained models on large datasets and fine-tune them on your low-resource data. This can help improve the performance of your model even with limited data.\n",
      "2. Data Augmentation: Create new data from existing data through techniques like synonym replacement, random insertion, random deletion, and random swap. This can help increase the size of your dataset and make it more robust.\n",
      "3. Active Learning: Selectively label a small number of instances from your dataset to maximize the information gain for your model. This can help improve the accuracy of your model with limited labeled data.\n",
      "4. Crowdsourcing: Collect data from a large crowd of people through platforms like Amazon Mechanical Turk. This can help you collect a large amount of data at a relatively low cost.\n",
      "5. Transfer Data From Related Domains: If there are related domains with more data available, you can transfer the data to your low-resource domain using techniques like domain adaptation.\n",
      "6. Create Synthetic Data: Use techniques like Generative Adversarial Networks (GANs) or Reinforcement Learning (RL) to create synthetic data that resembles your target distribution.\n",
      "7. Use Existing Datasets: If there are existing datasets that are relevant to your low-resource domain, you can use them to supplement your data. This can help increase the size and diversity of your dataset.\n",
      "8. Semi-Supervised Learning: Train your model on both labeled and unlabeled data using techniques like self-training or multi-view training. This can help improve the performance of your model with limited labeled data.\n",
      "\n",
      "Thought: Do I need to use a tool? No\n",
      "\n",
      "Final Answer: To implement these strategies, you may not necessarily need to use any specific tools, although there are several tools available that can help facilitate each strategy. For instance, for transfer learning, popular libraries like TensorFlow and PyTorch have pre-trained models that can be easily fine-tuned on your low-resource data. Similarly, for active learning, you can use libraries like Scikit-Learn or OpenCV to implement the active learning algorithms. For crowdsourcing, platforms like Amazon Mechanical Turk provide APIs and SDKs to collect and manage data from a large crowd of workers. For synthetic data generation, GANs and RL models can be implemented using TensorFlow, PyTorch, or other deep learning frameworks. For transferring data from related domains, you can use libraries like scikit-learn for domain adaptation techniques. And finally, for semi-supervised learning, popular libraries like TensorFlow and PyTorch support self-training and multi-view training algorithms.\n",
      "\n",
      "Therefore, the best way to create a dataset for low-resource NLP models would depend on the specifics of your use case, including the size and nature of your existing data, the domain you are working in, and the resources and tools available to you. Based on this analysis, I would recommend exploring the strategies outlined above and selecting the one(s) that best suit your needs. Additionally, you may want to consider using a combination of these strategies to maximize the impact of your efforts.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Task output: To implement these strategies, you may not necessarily need to use any specific tools, although there are several tools available that can help facilitate each strategy. For instance, for transfer learning, popular libraries like TensorFlow and PyTorch have pre-trained models that can be easily fine-tuned on your low-resource data. Similarly, for active learning, you can use libraries like Scikit-Learn or OpenCV to implement the active learning algorithms. For crowdsourcing, platforms like Amazon Mechanical Turk provide APIs and SDKs to collect and manage data from a large crowd of workers. For synthetic data generation, GANs and RL models can be implemented using TensorFlow, PyTorch, or other deep learning frameworks. For transferring data from related domains, you can use libraries like scikit-learn for domain adaptation techniques. And finally, for semi-supervised learning, popular libraries like TensorFlow and PyTorch support self-training and multi-view training algorithms.\n",
      "\n",
      "Therefore, the best way to create a dataset for low-resource NLP models would depend on the specifics of your use case, including the size and nature of your existing data, the domain you are working in, and the resources and tools available to you. Based on this analysis, I would recommend exploring the strategies outlined above and selecting the one(s) that best suit your needs. Additionally, you may want to consider using a combination of these strategies to maximize the impact of your efforts.\n",
      "Working Agent: Supervisor\n",
      "Starting Task: Supervise the researcher and make sure they are doing their job correctly. ...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: Do I need to use a tool? No\n",
      "Final Answer: Based on the context provided, I recommend exploring the strategies of transfer learning, active learning, crowdsourcing, synthetic data generation, and transferring data from related domains for creating a dataset for low-resource NLP models. The best approach will depend on the specifics of your use case, including the size and nature of your existing data, the domain you are working in, and the resources and tools available to you. Consider selecting one or more strategies to maximize the impact of your efforts.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Task output: Based on the context provided, I recommend exploring the strategies of transfer learning, active learning, crowdsourcing, synthetic data generation, and transferring data from related domains for creating a dataset for low-resource NLP models. The best approach will depend on the specifics of your use case, including the size and nature of your existing data, the domain you are working in, and the resources and tools available to you. Consider selecting one or more strategies to maximize the impact of your efforts.\n",
      "Working Agent: Writer\n",
      "Starting Task: Write a detailed methodology to improve the performance of LMs in data scarce scenarios. ...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: Do I need to use a tool? No\n",
      "\n",
      "Final Answer: To improve the performance of Language Models (LMs) in data scarce scenarios, I recommend exploring the following strategies based on the context provided:\n",
      "\n",
      "1. Transfer Learning: Use pre-trained models as a starting point and fine-tune them on your scarce data. This approach can help LMs generalize better to new tasks and domains with limited training data.\n",
      "2. Active Learning: Selectively label the most informative instances in your dataset to improve model performance iteratively. This method can be particularly useful when labeling data is time-consuming or expensive.\n",
      "3. Crowdsourcing: Leverage a large, distributed workforce to collect, annotate, and expand your dataset. This approach can help you gather diverse, high-quality data more efficiently than traditional methods.\n",
      "4. Synthetic Data Generation: Create synthetic data using techniques like data augmentation or GANs to increase the size and diversity of your existing dataset. This strategy can be useful when real data is scarce but labeled examples are available.\n",
      "5. Transferring Data from Related Domains: Adapt models trained on related domains by adapting them to new tasks or languages using techniques like domain adaptation, multitask learning, or fine-tuning. This approach can help you leverage existing knowledge and resources for your low-resource NLP application.\n",
      "\n",
      "Select one or more strategies based on the specifics of your use case, including the size and nature of your existing data, the domain you are working in, and the resources and tools available to you. Combining multiple strategies could lead to even better performance improvements.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Task output: To improve the performance of Language Models (LMs) in data scarce scenarios, I recommend exploring the following strategies based on the context provided:\n",
      "\n",
      "1. Transfer Learning: Use pre-trained models as a starting point and fine-tune them on your scarce data. This approach can help LMs generalize better to new tasks and domains with limited training data.\n",
      "2. Active Learning: Selectively label the most informative instances in your dataset to improve model performance iteratively. This method can be particularly useful when labeling data is time-consuming or expensive.\n",
      "3. Crowdsourcing: Leverage a large, distributed workforce to collect, annotate, and expand your dataset. This approach can help you gather diverse, high-quality data more efficiently than traditional methods.\n",
      "4. Synthetic Data Generation: Create synthetic data using techniques like data augmentation or GANs to increase the size and diversity of your existing dataset. This strategy can be useful when real data is scarce but labeled examples are available.\n",
      "5. Transferring Data from Related Domains: Adapt models trained on related domains by adapting them to new tasks or languages using techniques like domain adaptation, multitask learning, or fine-tuning. This approach can help you leverage existing knowledge and resources for your low-resource NLP application.\n",
      "\n",
      "Select one or more strategies based on the specifics of your use case, including the size and nature of your existing data, the domain you are working in, and the resources and tools available to you. Combining multiple strategies could lead to even better performance improvements.\n"
     ]
    }
   ],
   "source": [
    "result = crew.kickoff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CREWAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
